{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWa2qZeqdMkLAnw0bXLGpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kn0wthing/practice/blob/main/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-DwK5Q_lGaNQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DcyCg1aGmvu",
        "outputId": "9538652e-fb83-463c-9caa-59d216ccd6b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 20.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 303kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.61MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.84MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 100"
      ],
      "metadata": {
        "id": "9JRT1nu3UamK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                            ])\n",
        "trainset.transform = transformations\n",
        "testset.transform = transformations"
      ],
      "metadata": {
        "id": "CT6-cL2DJXRp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size_train, shuffle=True)"
      ],
      "metadata": {
        "id": "DgvChrGDcl2o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.data.shape, testset.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfmXBvdzc_7D",
        "outputId": "955d886b-c5ec-410f-9c1d-cd906dd40503"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "UTgIsmVGTDj1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total parameters: {total_params}\")\n",
        "    print(f\"Trainable parameters: {trainable_params}\")\n",
        "    return total_params, trainable_params"
      ],
      "metadata": {
        "id": "2AQKI97jUx0E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "#         self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "#         self.dropout1 = nn.Dropout(0.25)\n",
        "#         self.dropout2 = nn.Dropout(0.5)\n",
        "#         self.fc1 = nn.Linear(9216, 128)\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = F.max_pool2d(x, 2)\n",
        "#         x = self.dropout1(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.dropout2(x)\n",
        "#         x = self.fc2(x)\n",
        "#         output = F.log_softmax(x, dim=1)\n",
        "#         return output\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "cnn_model = Net()\n",
        "optimizer = optim.SGD(cnn_model.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)\n",
        "count_parameters(cnn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OEy814ITI98",
        "outputId": "9fa79ee5-52d3-48e5-9b0f-1c91c9c362cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 21840\n",
            "Trainable parameters: 21840\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21840, 21840)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inspired from seperable convolutions https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
        "        self.inst_norm1 = nn.InstanceNorm2d(8)\n",
        "\n",
        "        self.separable_conv1 = SeparableConv2d(8, 32, kernel_size=3)\n",
        "        self.inst_norm2 = nn.InstanceNorm2d(32)\n",
        "\n",
        "        self.separable_conv2 = SeparableConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.inst_norm3 = nn.InstanceNorm2d(32)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc1 = nn.Linear(32, 16)\n",
        "        self.inst_norm4 = nn.InstanceNorm1d(16)\n",
        "        self.fc2 = nn.Linear(16, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.inst_norm1(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.separable_conv1(x))\n",
        "        x = self.inst_norm2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.relu(self.separable_conv2(x))\n",
        "        x = self.inst_norm3(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.inst_norm4(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CustomModel()\n",
        "print(model)\n",
        "\n",
        "# Example usage with your model\n",
        "# model = Model()\n",
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10m7D7J5dEAx",
        "outputId": "d2b6b975-5776-48ee-c1a4-5b75dda46596"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomModel(\n",
            "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (inst_norm1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  (separable_conv1): SeparableConv2d(\n",
            "    (depthwise): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=8)\n",
            "    (pointwise): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (inst_norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  (separable_conv2): SeparableConv2d(\n",
            "    (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
            "    (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (inst_norm3): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (inst_norm4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "  (fc2): Linear(in_features=16, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Total parameters: 2522\n",
            "Trainable parameters: 2522\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2522, 2522)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "def train(epoch, cnn_model):\n",
        "  cnn_model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = cnn_model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "\n",
        "def test(cnn_model):\n",
        "  cnn_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = cnn_model(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "# test(cnn_model)\n",
        "# for epoch in range(1, n_epochs + 1):\n",
        "#   train(epoch, cnn_model)\n",
        "#   test(cnn_model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rm_wbpTtjCX8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "test(model)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch, model)\n",
        "  test(model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CFuKs_ryh3kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rep87AuWHUZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}